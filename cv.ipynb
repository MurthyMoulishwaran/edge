{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6457d3b6-3ed7-496f-9638-a6af8b0aed6d",
   "metadata": {},
   "source": [
    "## 9.water shed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8981da8-7fa8-4c56-aec1-00d949f06075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "I = cv2.imread(\"/content/image25.jpeg\")\n",
    "\n",
    "# Check if the image is read correctly\n",
    "if I is None:\n",
    "    raise FileNotFoundError('Could not open or find the image.')\n",
    "\n",
    "# Step 2: Convert to Grayscale (if necessary)\n",
    "if len(I.shape) == 3:\n",
    "    I = cv2.cvtColor(I, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 3: Noise Reduction\n",
    "I_filtered = cv2.GaussianBlur(I, (5, 5), 2)  # Apply Gaussian filter to reduce noise\n",
    "\n",
    "# Step 4: Compute the Gradient Magnitude\n",
    "sobelx = cv2.Sobel(I_filtered, cv2.CV_64F, 1, 0, ksize=3)\n",
    "sobely = cv2.Sobel(I_filtered, cv2.CV_64F, 0, 1, ksize=3)\n",
    "gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "\n",
    "# Step 5: Marker-Based Segmentation\n",
    "_, binary_image = cv2.threshold(I_filtered, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "D = ndi.distance_transform_edt(binary_image)\n",
    "local_max = cv2.dilate(cv2.erode(D, None), None)\n",
    "markers, _ = ndi.label(local_max)\n",
    "gradmag2 = cv2.watershed(cv2.cvtColor((gradmag * 255 / gradmag.max()).astype(np.uint8), cv2.COLOR_GRAY2BGR), markers)\n",
    "\n",
    "# Step 6: Apply the Watershed Transform\n",
    "labels = cv2.watershed(cv2.cvtColor(I_filtered, cv2.COLOR_GRAY2BGR), gradmag2)\n",
    "\n",
    "# Step 7: Define 7 distinct colors for visualization\n",
    "colors = [\n",
    "    [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0],\n",
    "    [0, 255, 255], [255, 0, 255], [192, 192, 192]\n",
    "]\n",
    "\n",
    "# Create an empty image for the result\n",
    "Lrgb = np.zeros((*labels.shape, 3), dtype=np.uint8)\n",
    "\n",
    "# Color the regions\n",
    "for label in np.unique(labels):\n",
    "    if label == -1:  # Ignore background\n",
    "        continue\n",
    "    mask = labels == label\n",
    "    color_idx = (label - 1) % 7\n",
    "    Lrgb[mask] = colors[color_idx]\n",
    "\n",
    "# Step 8: Visualize the Results\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(I, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Gradient Magnitude\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(gradmag, cmap='gray')\n",
    "plt.title('Gradient Magnitude')\n",
    "plt.axis('off')\n",
    "\n",
    "# Binary Image\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(binary_image, cmap='gray')\n",
    "plt.title('Binary Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Watershed Transform\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(Lrgb)\n",
    "plt.title('Watershed Transform')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d0224-f9bb-42f0-a78e-fb950a821066",
   "metadata": {},
   "source": [
    "## 11.CNN with and without pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e952d-6049-4c85-86cc-ab0f356e0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "image_path = '/content/drive/MyDrive/de975595890f0ed79238dc4d61532777.jpg'\n",
    "\n",
    "def extract_features_with_cnn(image_path):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    # Extract features from an earlier layer for even higher resolution\n",
    "    model = Model(inputs=base_model.inputs, outputs=base_model.get_layer('block2_pool').output)\n",
    "\n",
    "    img = cv.imread(image_path)\n",
    "    img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    img_resized = cv.resize(img_rgb, (224, 224))\n",
    "    img_array = np.expand_dims(img_resized, axis=0)\n",
    "    img_preprocessed = preprocess_input(img_array)\n",
    "\n",
    "    features = model.predict(img_preprocessed)\n",
    "    features_reshaped = features.reshape(-1, features.shape[-1])\n",
    "\n",
    "    return features_reshaped, img_rgb, features.shape[1:3]  # Return spatial dimensions\n",
    "\n",
    "def kmeans_with_and_without_pca(image_path, cluster_n):\n",
    "    features, img_rgb, feature_map_size = extract_features_with_cnn(image_path)\n",
    "\n",
    "    # KMeans clustering without PCA\n",
    "    kmeans_no_pca = KMeans(n_clusters=cluster_n, random_state=42)\n",
    "    labels_no_pca = kmeans_no_pca.fit_predict(features)\n",
    "\n",
    "    silhouette_no_pca = silhouette_score(features, labels_no_pca)\n",
    "    print(f\"Silhouette Score without PCA: {silhouette_no_pca:.4f}\")\n",
    "\n",
    "    # KMeans clustering with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    features_pca = pca.fit_transform(features)\n",
    "\n",
    "    kmeans_pca = KMeans(n_clusters=cluster_n, random_state=42)\n",
    "    labels_pca = kmeans_pca.fit_predict(features_pca)\n",
    "\n",
    "    silhouette_pca = silhouette_score(features_pca, labels_pca)\n",
    "    print(f\"Silhouette Score with PCA: {silhouette_pca:.4f}\")\n",
    "\n",
    "    # Segmented image visualization (no PCA)\n",
    "    segmented_img = labels_no_pca.reshape(feature_map_size)  # Reshape labels to feature map size\n",
    "\n",
    "    # Resize segmented image back to original image size\n",
    "    segmented_img_resized = cv.resize(segmented_img.astype(np.uint8), (img_rgb.shape[1], img_rgb.shape[0]), interpolation=cv.INTER_LINEAR)\n",
    "\n",
    "    # Apply a bilateral filter for edge-aware smoothing (preserves edges)\n",
    "    segmented_img_filtered = cv.bilateralFilter(segmented_img_resized, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Plotting the results in a 2x2 grid with reduced size\n",
    "    plt.figure(figsize=(10, 8))  # Reduced figsize\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title('Input Image')\n",
    "\n",
    "    # PCA feature plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], c=labels_pca, cmap='tab10', s=20)\n",
    "    plt.colorbar(scatter, ticks=range(cluster_n), label='Cluster')\n",
    "    plt.title('PCA of CNN-extracted Features (with K-means Clustering)')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "\n",
    "    # Segmented image\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(segmented_img_filtered, cmap='tab10')\n",
    "    plt.axis('off')\n",
    "    plt.title('Clearer Segmented Image with Smoothing')\n",
    "\n",
    "    # Silhouette score comparison plot\n",
    "    plt.subplot(2, 2, 4)\n",
    "    scores = [silhouette_no_pca, silhouette_pca]\n",
    "    labels = ['Without PCA', 'With PCA']\n",
    "    plt.bar(labels, scores, color=['skyblue', 'salmon'])\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Comparison of Silhouette Scores')\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Running the KMeans clustering with CNN features\n",
    "cluster_n = 5\n",
    "kmeans_with_and_without_pca(image_path, cluster_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a38b5-0be8-483f-93d0-1afd7e2b5f02",
   "metadata": {},
   "source": [
    "## 4.edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e711491-e43e-4fe8-be48-67310de00622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the original image\n",
    "img = cv2.imread(\"/content/drive/MyDrive/cat.jpeg\", 1)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(\"The specified image path is incorrect or the image cannot be found.\")\n",
    "original_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Gaussian Blur\n",
    "img_gaussian = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "# Canny Edge Detection\n",
    "img_canny = cv2.Canny(img, 100, 200)\n",
    "\n",
    "# Sobel Edge Detection\n",
    "img_sobelx = cv2.Sobel(img_gaussian, cv2.CV_8U, 1, 0, ksize=5)\n",
    "img_sobely = cv2.Sobel(img_gaussian, cv2.CV_8U, 0, 1, ksize=5)\n",
    "img_sobel = img_sobelx + img_sobely\n",
    "\n",
    "# Prewitt Edge Detection\n",
    "kernelx = np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]])\n",
    "kernely = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
    "img_prewittx = cv2.filter2D(img_gaussian, -1, kernelx)\n",
    "img_prewitty = cv2.filter2D(img_gaussian, -1, kernely)\n",
    "img_prewitt = img_prewittx + img_prewitty\n",
    "\n",
    "# Laplacian Edge Detection\n",
    "laplacian = cv2.Laplacian(img_gaussian, cv2.CV_64F)\n",
    "\n",
    "# LoG (Laplacian of Gaussian)\n",
    "img2 = cv2.imread(\"/content/drive/MyDrive/cat.jpeg\")\n",
    "if img2 is None:\n",
    "    raise FileNotFoundError(\"The specified image path is incorrect or the image cannot be found.\")\n",
    "gray_img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "LoG = nd.gaussian_laplace(gray_img2, 2)\n",
    "thres = np.absolute(LoG).mean() * 0.75\n",
    "output = np.zeros(LoG.shape)\n",
    "\n",
    "# Zero-crossing detection for LoG\n",
    "w = output.shape[1]\n",
    "h = output.shape[0]\n",
    "for y in range(1, h - 1):\n",
    "    for x in range(1, w - 1):\n",
    "        patch = LoG[y - 1:y + 2, x - 1:x + 2]\n",
    "        p = LoG[y, x]\n",
    "        maxP = patch.max()\n",
    "        minP = patch.min()\n",
    "        if (p > 0).any():\n",
    "            zeroCross = True if minP < 0 else False\n",
    "        else:\n",
    "            zeroCross = True if maxP > 0 else False\n",
    "        if ((maxP - minP) > thres) and zeroCross:\n",
    "            output[y, x] = 1\n",
    "\n",
    "# DoG (Difference of Gaussian)\n",
    "A = np.array([[0, 0, -1, -1, -1, 0, 0],\n",
    "              [0, -2, -3, -3, -3, -2, 0],\n",
    "              [-1, -3, 5, 5, 5, -3, -1],\n",
    "              [-1, -3, 5, 16, 5, -3, -1],\n",
    "              [-1, -3, 5, 5, 5, -3, -1],\n",
    "              [0, -2, -3, -3, -3, -2, 0],\n",
    "              [0, 0, -1, -1, -1, 0, 0]], dtype=np.float32)\n",
    "\n",
    "ratio = img2.shape[0] / 500.0\n",
    "new_width = int(img2.shape[1] / ratio)\n",
    "original = img2.copy()\n",
    "nimg = cv2.resize(gray, (new_width, 500))\n",
    "\n",
    "I1 = signal.convolve2d(nimg, A)\n",
    "I1 = np.absolute(I1)\n",
    "I1 = (I1 - np.min(I1)) / float(np.max(I1) - np.min(I1))\n",
    "\n",
    "I2 = cv2.GaussianBlur(I1, (5, 5), 0)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 4, 1), plt.imshow(original_img)\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2, 4, 2), plt.imshow(gray, cmap='gray')\n",
    "plt.title('Gray Scale Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2, 4, 3), plt.imshow(img_canny, cmap='gray')\n",
    "plt.title('Canny Edge Detection'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2, 4, 4), plt.imshow(img_sobel, cmap='gray')\n",
    "plt.title('Sobel Edge Detection'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2, 4, 5), plt.imshow(img_prewitt, cmap='gray')\n",
    "plt.title('Prewitt Edge Detection'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2, 4, 6), plt.imshow(laplacian, cmap='gray')\n",
    "plt.title('Laplacian Edge Detection'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2, 4, 7), plt.imshow(output, cmap='gray')\n",
    "plt.title('LoG Edge Detection'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2, 4, 8), plt.imshow(I2, cmap='gray')\n",
    "plt.title('DoG Edge Detection'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e65423-66d9-4410-82ee-4af9849869be",
   "metadata": {},
   "source": [
    "## 6.houghline transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375854b-2def-411f-bf5e-d66ccdaadc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "\n",
    "img = cv2.imread(\"/content/geometric-shapes-clip-art-294298.png\")\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perform Canny edge detection\n",
    "edges = cv2.Canny(gray, 50, 150)  # Adjusted threshold values\n",
    "\n",
    "# Parameters for Hough transform\n",
    "minLineLength = 3\n",
    "maxLineGap = 1\n",
    "\n",
    "# Perform Hough line detection\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 50, minLineLength, maxLineGap)  # Adjusted parameters\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Display grayscale image\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.title('Grayscale Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display edge-detected image\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.title('Edges')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display the original image with Hough lines overlayed\n",
    "plt.subplot(2, 2, (3, 4))\n",
    "img_with_lines = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "if lines is not None:\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        cv2.line(img_with_lines, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw lines on the image\n",
    "plt.imshow(img_with_lines)\n",
    "plt.title('Hough Lines')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524bbec8-9843-462f-8e4d-262bcd8262ec",
   "metadata": {},
   "source": [
    "## 8.brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dea114-93fe-4df9-afd8-148a95b6a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# File paths from your Google Drive\n",
    "img1 = cv.imread('/content/drive/MyDrive/brute force image/eat the frog 1.jpg', cv.IMREAD_GRAYSCALE)\n",
    "img2 = cv.imread('/content/drive/MyDrive/brute force image/eat the frog 2.jpg', cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Check if images loaded correctly\n",
    "if img1 is None or img2 is None:\n",
    "    print(\"Error: Could not load one or both images.\")\n",
    "else:\n",
    "    # Print the OpenCV version\n",
    "    print('OpenCV Version:', cv.__version__)\n",
    "\n",
    "    # Initiate SIFT detector\n",
    "    sift = cv.SIFT_create()\n",
    "\n",
    "    # Find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)  # Or pass an empty dictionary\n",
    "\n",
    "    flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # Create a mask to draw only good matches\n",
    "    matchesMask = [[0, 0] for i in range(len(matches))]\n",
    "\n",
    "    # Ratio test as per Lowe's paper\n",
    "    for i, (m, n) in enumerate(matches):\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            matchesMask[i] = [1, 0]\n",
    "\n",
    "    # Draw parameters for matches\n",
    "    draw_params = dict(matchColor=(0, 255, 0),  # Draw good matches in green\n",
    "                       singlePointColor=(255, 0, 0),  # Draw keypoints in red\n",
    "                       matchesMask=matchesMask,  # Draw only matches that passed the ratio test\n",
    "                       flags=cv.DrawMatchesFlags_DEFAULT)\n",
    "\n",
    "    # Draw the matches\n",
    "    img3 = cv.drawMatchesKnn(img1, kp1, img2, kp2, matches, None, **draw_params)\n",
    "\n",
    "    # Display the matched image\n",
    "    plt.imshow(img3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658032b-f76f-46ee-97ff-41a6d67074e8",
   "metadata": {},
   "source": [
    "## 12.RNN and faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea90769-f42d-421e-bbdc-068f0797ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.ops import box_iou\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.colab import files\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "image_path = \"/content/cat.jpeg\"\n",
    "image = cv2.imread(image_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image_tensor = transform(image_rgb).unsqueeze(0)\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "image_tensor = image_tensor.to(device)\n",
    "\n",
    "ground_truth_boxes = torch.tensor([[50, 50, 200, 200]]).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)[0]\n",
    "\n",
    "pred_boxes = output['boxes']\n",
    "pred_scores = output['scores']\n",
    "\n",
    "iou_threshold = 0.5\n",
    "true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "\n",
    "if len(pred_boxes) > 0 and len(ground_truth_boxes) > 0:\n",
    "    ious = box_iou(pred_boxes, ground_truth_boxes)\n",
    "\n",
    "    for i, iou_row in enumerate(ious):\n",
    "        max_iou, _ = iou_row.max(0)\n",
    "        if max_iou >= iou_threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "    false_negatives += len(ground_truth_boxes) - len(ious)\n",
    "\n",
    "else:\n",
    "    false_negatives += len(ground_truth_boxes)\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "for box in pred_boxes:\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "cv2_imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaae58d-a8fc-4ae9-a74c-59008fa08844",
   "metadata": {},
   "source": [
    "## 10.kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd33d5-6568-4fdd-ba8f-e2a1b96cb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Read the image using OpenCV\n",
    "image = cv2.imread(\"/content/drive/MyDrive/cat.jpeg\")\n",
    "\n",
    "# Convert BGR to RGB\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Reshape the image to a 2D array of pixels (each pixel is a vector of 3 values: R, G, B)\n",
    "pixels = image_rgb.reshape((-1, 3))\n",
    "pixels = np.float32(pixels)\n",
    "\n",
    "# Define criteria and apply KMeans\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "k = 4  # Number of clusters\n",
    "_, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "# Convert centers to uint8 (color values)\n",
    "centers = np.uint8(centers)\n",
    "\n",
    "# Map labels to the corresponding centers (cluster centers)\n",
    "segmented_image = centers[labels.flatten()]\n",
    "\n",
    "# Reshape back to the original image size\n",
    "segmented_image = segmented_image.reshape(image_rgb.shape)\n",
    "\n",
    "# Calculate the number of pixels in each cluster\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# Set up the figure for multiple subplots\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "# Plot original image\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "ax1.imshow(image_rgb)\n",
    "ax1.set_title('Original Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Plot segmented image\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax2.imshow(segmented_image)\n",
    "ax2.set_title('Segmented Image (K={})'.format(k))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Create and display a simple pivot-like table with pixel counts per cluster\n",
    "ax_pivot = fig.add_subplot(2, 3, 3)\n",
    "table_data = list(zip(unique, counts))\n",
    "col_labels = ['Cluster', 'Pixel Count']\n",
    "\n",
    "# Create a table in the plot\n",
    "ax_pivot.axis('tight')\n",
    "ax_pivot.axis('off')\n",
    "table = ax_pivot.table(cellText=table_data, colLabels=col_labels, loc='center')\n",
    "table.set_fontsize(12)\n",
    "table.scale(1, 1.5)\n",
    "ax_pivot.set_title(\"Pixel Count per Cluster\")\n",
    "\n",
    "# Plot the 3D scatter diagram for clustering in RGB space\n",
    "ax3 = fig.add_subplot(2, 3, (4, 6), projection='3d')\n",
    "r = pixels[:, 0]\n",
    "g = pixels[:, 1]\n",
    "b = pixels[:, 2]\n",
    "\n",
    "# Use labels to color the pixels in the scatter plot\n",
    "ax3.scatter(r, g, b, c=labels.flatten(), s=1, cmap='viridis')\n",
    "ax3.set_xlabel('Red')\n",
    "ax3.set_ylabel('Green')\n",
    "ax3.set_zlabel('Blue')\n",
    "ax3.set_title('3D RGB Clustering Plot (K={})'.format(k))\n",
    "\n",
    "# Show the final plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5afcb-805d-4283-bd23-795f9e68ef78",
   "metadata": {},
   "source": [
    "## 7.harris corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963be4f0-9a7d-4656-be77-c78910f44896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"/content/OIP.jpeg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for matplotlib display\n",
    "\n",
    "# Display original image\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "gray = np.float32(gray)\n",
    "\n",
    "# Apply Harris Corner Detection\n",
    "dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "\n",
    "# Result is dilated for marking the corners\n",
    "dst = cv2.dilate(dst, None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image\n",
    "img[dst > 0.01 * dst.max()] = [255, 0, 0]\n",
    "\n",
    "# Display result\n",
    "plt.imshow(img)\n",
    "plt.title('Harris Corner Detection')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb9289-9910-4b04-b58c-dbdf0b3d6793",
   "metadata": {},
   "source": [
    "## 5.histogram computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d4297-146e-4306-a90b-c74bee4f24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread('/cat.jpeg')\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Unable to read the image file.\")\n",
    "else:\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    images = [gray_image, image]\n",
    "    titles = ['Grayscale Image', 'Color Image']\n",
    "    channels = ['Grayscale', 'Blue', 'Green', 'Red']\n",
    "    colors = ['black', 'blue', 'green', 'red']\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for i in range(2):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        if i == 0:\n",
    "            plt.imshow(images[i], cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB))\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    for j, (channel, color) in enumerate(zip(channels, colors)):\n",
    "        plt.subplot(2, 3, j + 3)\n",
    "        if channel == 'Grayscale':\n",
    "            hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "        else:\n",
    "            hist = cv2.calcHist([image], [j - 1], None, [256], [0, 256])\n",
    "        plt.plot(hist, color=color)\n",
    "        plt.title(f'{channel} Histogram')\n",
    "        plt.xlabel('Bins')\n",
    "        plt.ylabel('# of Pixels')\n",
    "        plt.xlim([0, 256])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfddc3d-0f95-4169-b01e-9f3eb54b37e6",
   "metadata": {},
   "source": [
    "## 3.morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cf486-5efd-4a28-b93c-3a47c952f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read image\n",
    "img = cv.imread(\"/content/cat.jpeg\", 1)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Apply morphological operations\n",
    "erosion = cv.erode(img, kernel, iterations=1)\n",
    "dilation = cv.dilate(img, kernel, iterations=1)\n",
    "opening = cv.morphologyEx(img, cv.MORPH_OPEN, kernel)\n",
    "closing = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel)\n",
    "gradient = cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel)\n",
    "tophat = cv.morphologyEx(img, cv.MORPH_TOPHAT, kernel)\n",
    "blackhat = cv.morphologyEx(img, cv.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "# Create a list of titles and corresponding images\n",
    "titles = ['Original', 'Erosion', 'Dilation', 'Opening', 'Closing', 'Gradient', 'Tophat', 'Blackhat']\n",
    "images = [img, erosion, dilation, opening, closing, gradient, tophat, blackhat]\n",
    "\n",
    "# Plot all images in a single figure\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(2, 4, i + 1)  # 2 rows, 4 columns\n",
    "    plt.imshow(cv.cvtColor(images[i], cv.COLOR_BGR2RGB))\n",
    "    plt.title(titles[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa04b5-6ad8-4b22-aeb0-1952b32cc733",
   "metadata": {},
   "source": [
    "## grapcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a1c33-a78c-41dd-8d86-612a19e5f946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ffb4f-27fc-4bbf-abf5-147baba1f3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db694606-f7cf-4b67-9ffb-725d525ee502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f23fa5-b6a8-41f1-86e1-216c62009327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4313393-30fe-4690-8896-a213f91cc0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dee92-bc1f-46b2-ae6c-1199bdafbfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee7c4f-3a64-4075-bf92-02c045e42595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb31ef-c9ec-46d6-9597-599fe7823bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21e0ef-4aaf-4cfb-adbc-5549ce251421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e927d79-b4ca-4e89-8c2e-dabc6981e5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b416ab-a86c-43f7-a4d5-e7cd55cdc288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63836915-fef2-4a0c-9967-fa58b5bcecdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c68fde-98eb-4b35-a999-6db2f0a6a07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d71e13-1a39-4ef5-85ac-42245bd9e090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c21c48-59f8-4593-846d-2cc8a4caf75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c605483-eb7d-4d9d-ab83-abeb75b5148f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd96fba-b544-4433-9c66-bde04eff702a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
