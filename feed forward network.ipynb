{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af2ba7a-f65c-47f2-a4d6-5d5de3d8eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f49d432-f0e9-4b03-9f04-8cb3cca97286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.api.datasets.mnist' from 'D:\\\\users\\\\Lib\\\\site-packages\\\\keras\\\\api\\\\datasets\\\\mnist\\\\__init__.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "388d0264-def0-41d6-8744-d7ece21c486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4690cc2d-622b-440d-8fca-5c34206d7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9980dbcd-05e3-4cf4-9daa-b197b659af87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\users\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(28*28,)),  \n",
    "    layers.Dense(64, activation='relu'),  \n",
    "    layers.Dense(10, activation='softmax') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae08ae8-b227-4b9b-8fd7-b72ed646d011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential, built=True>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "321f04f4-2b3c-4c6b-8b47-de0964f22946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8637 - loss: 0.4751 - val_accuracy: 0.9596 - val_loss: 0.1378\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9641 - loss: 0.1192 - val_accuracy: 0.9590 - val_loss: 0.1309\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9769 - loss: 0.0756 - val_accuracy: 0.9706 - val_loss: 0.1018\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9844 - loss: 0.0536 - val_accuracy: 0.9716 - val_loss: 0.0971\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9859 - loss: 0.0453 - val_accuracy: 0.9713 - val_loss: 0.0992\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.0304 - val_accuracy: 0.9699 - val_loss: 0.1076\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.0271 - val_accuracy: 0.9747 - val_loss: 0.0923\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0208 - val_accuracy: 0.9733 - val_loss: 0.1009\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9940 - loss: 0.0181 - val_accuracy: 0.9764 - val_loss: 0.1087\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0179 - val_accuracy: 0.9745 - val_loss: 0.1098\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c749e46c-e074-4ab5-8a16-f23acd5afe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9743 - loss: 0.1030\n",
      "Test accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0e2c4fa-3c8c-4108-94ab-3ff2d7137a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape the flattened image (784,) back to its original shape (28, 28)\n",
    "image = X_train[5].reshape(28, 28)\n",
    "\n",
    "# Display the reshaped image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Label: {y_train[0]}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "380e5b24-4118-465a-81d6-7dfc60ebf1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 557 images belonging to 14 classes.\n",
      "Found 137 images belonging to 14 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\users\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "D:\\users\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523ms/step - accuracy: 0.0648 - loss: 2.7560"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\users\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 693ms/step - accuracy: 0.0645 - loss: 2.7526 - val_accuracy: 0.0949 - val_loss: 2.6373\n",
      "Epoch 2/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 497ms/step - accuracy: 0.1221 - loss: 2.6328 - val_accuracy: 0.1241 - val_loss: 2.6228\n",
      "Epoch 3/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 501ms/step - accuracy: 0.1596 - loss: 2.5819 - val_accuracy: 0.1825 - val_loss: 2.6023\n",
      "Epoch 4/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 530ms/step - accuracy: 0.2182 - loss: 2.3871 - val_accuracy: 0.2263 - val_loss: 2.5039\n",
      "Epoch 5/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 499ms/step - accuracy: 0.3448 - loss: 2.1401 - val_accuracy: 0.2263 - val_loss: 2.5171\n",
      "Epoch 6/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 523ms/step - accuracy: 0.4341 - loss: 1.9003 - val_accuracy: 0.2555 - val_loss: 2.6451\n",
      "Epoch 7/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 521ms/step - accuracy: 0.5001 - loss: 1.6036 - val_accuracy: 0.3869 - val_loss: 2.3999\n",
      "Epoch 8/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 498ms/step - accuracy: 0.6374 - loss: 1.2555 - val_accuracy: 0.3869 - val_loss: 2.6558\n",
      "Epoch 9/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 533ms/step - accuracy: 0.6719 - loss: 1.0652 - val_accuracy: 0.4234 - val_loss: 2.5389\n",
      "Epoch 10/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 540ms/step - accuracy: 0.8290 - loss: 0.6507 - val_accuracy: 0.4161 - val_loss: 2.6361\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Predicted dog breed: {'Affenhuahua dog': 0, 'Afgan Hound dog': 1, 'Akita dog': 2, 'Alaskan Malamute dog': 3, 'American Bulldog dog': 4, 'Auggie dog': 5, 'Beagle dog': 6, 'Belgian Tervuren dog': 7, 'Bichon Frise dog': 8, 'Bocker dog': 9, 'Borzoi dog': 10, 'Boxer dog': 11, 'Bugg dog': 12, 'Bulldog dog': 13}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image_size = (128, 128) \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  \n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r\"D:\\3 rd sem\\dl datasets\\dog dataset\", \n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical', \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    r\"D:\\3 rd sem\\dl datasets\\dog dataset\",  \n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Step 2: Build the Convolutional Neural Network (CNN)\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(train_generator.num_classes, activation='softmax')  # Adjust to the number of dog breeds\n",
    "])\n",
    "\n",
    "# Step 3: Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Step 5: Predict a single image\n",
    "# Load a single image and preprocess it\n",
    "img_path = r\"C:\\Users\\Moulishwaran\\OneDrive\\Pictures\\Dog image\\image11.jpeg\"\n",
    "img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# Predict the breed of the dog\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the predicted class\n",
    "print(f\"Predicted dog breed: {train_generator.class_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "528cd638-b6ff-4552-87dc-6a3018744f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 694 images belonging to 14 classes.\n",
      "Found 694 images belonging to 14 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 893ms/step - accuracy: 0.0784 - loss: 30.6988 - val_accuracy: 0.0699 - val_loss: 7.1340\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0625 - loss: 7.9407 - val_accuracy: 0.0000e+00 - val_loss: 9.1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\users\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 813ms/step - accuracy: 0.0653 - loss: 5.7057 - val_accuracy: 0.1116 - val_loss: 3.3868\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0312 - loss: 3.1562 - val_accuracy: 0.0455 - val_loss: 2.9748\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 867ms/step - accuracy: 0.0882 - loss: 3.2491 - val_accuracy: 0.0774 - val_loss: 3.2153\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0625 - loss: 3.5206 - val_accuracy: 0.0000e+00 - val_loss: 3.2985\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 790ms/step - accuracy: 0.0703 - loss: 3.1667 - val_accuracy: 0.1027 - val_loss: 2.8083\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 2.9971 - val_accuracy: 0.1818 - val_loss: 2.6644\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 793ms/step - accuracy: 0.0826 - loss: 2.9261 - val_accuracy: 0.0997 - val_loss: 3.2832\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0312 - loss: 3.3706 - val_accuracy: 0.1364 - val_loss: 3.3035\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 333ms/step - accuracy: 0.0789 - loss: 3.0958\n",
      "Validation accuracy: 0.0836\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_height, img_width = 150, 150  \n",
    "batch_size = 32\n",
    "train_data_dir = r\"D:\\3 rd sem\\dl datasets\\dog dataset\"  \n",
    "validation_data_dir = r\"D:\\3 rd sem\\dl datasets\\dog dataset\"  \n",
    "num_classes = 14 \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0, \n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  \n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  \n",
    ")\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(img_height, img_width, 3)))  \n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax')) \n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_generator, \n",
    "          steps_per_epoch=train_generator.samples // batch_size,\n",
    "          epochs=10, \n",
    "          validation_data=validation_generator, \n",
    "          validation_steps=validation_generator.samples // batch_size)\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863fbf3-35e1-42d7-9248-f78a66f01ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea65cd-189c-41b6-b830-5a74df531ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
