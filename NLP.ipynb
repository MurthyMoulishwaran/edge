{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b49616-e956-4493-87c4-6c0ad9af7399",
   "metadata": {},
   "source": [
    "## 1.tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb5a95a-8ea4-4054-88ad-5fa0d79265fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Hi', ',', 'I', 'am', 'Moulish']\n",
      "Sentences: ['Hi, I am Moulish']\n",
      "Space: ['Hi,', 'I', 'am', 'Moulish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize,SpaceTokenizer\n",
    "\n",
    "text = \"Hi, I am Moulish\"\n",
    "words = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "space = SpaceTokenizer().tokenize(text)\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Space:\", space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c9080-f593-4321-bf2c-e3059a6d363f",
   "metadata": {},
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655ed721-cf21-4cc5-8b8d-f07b85fff050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'studying', 'at', 'periyar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize as wt\n",
    "words=\"I am studying at periyar\"\n",
    "words=wt(words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1256f778-84a4-409c-9106-20f1d5c3d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ddf9818-dd1e-4703-af6b-593798f87c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0228a8ca-c49b-48b3-9ba3-551a791af406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ---> I\n",
      "am ---> am\n",
      "studying ---> studying\n",
      "at ---> at\n",
      "periyar ---> periyar\n"
     ]
    }
   ],
   "source": [
    "list1 = words\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + wnl.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9db3256-9d85-44c5-a675-35cde9ad06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runnings', 'file', 'better', 'jump']\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "words=[\"runnings\",\"files\",\"better\",\"jumping\"]\n",
    "lemma = [lemmatizer.lemmatize(word,pos=\"v\") for word in words]\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3e788-2091-492c-ab4f-5e8cfd419bff",
   "metadata": {},
   "source": [
    "# 2.stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e554878e-0f02-44a1-b3b0-28a30f6867f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['i', 'am', 'moulish', ',', 'i', 'went', 'to', 'college', 'for', 'studying']\n",
      "Stemmed words: ['i', 'am', 'moulish', ',', 'i', 'went', 'to', 'colleg', 'for', 'studi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer,LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"i am moulish, i went to college for studying \"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f40d6e-7901-443a-b817-0ff14af08e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['i', 'am', 'moulish', ',', 'i', 'went', 'to', 'college', 'for', 'studying']\n",
      "Stemmed words: ['i', 'am', 'moulish', ',', 'i', 'went', 'to', 'colleg', 'for', 'studi']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "128dce0c-5cdf-47c7-9450-30ecf97f7313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'moul', ',', 'i', 'went', 'to', 'colleg', 'for', 'study']\n"
     ]
    }
   ],
   "source": [
    "ls=LancasterStemmer()\n",
    "stem_word= [ls.stem(word) for word in words]\n",
    "print(stem_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea4e98-0228-4dba-93b5-8a74fd194b8e",
   "metadata": {},
   "source": [
    "# sentence Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e676f-0657-431f-85b1-08f1e198bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d0377-854d-4dce-969c-d7bfdbfe3480",
   "metadata": {},
   "source": [
    "# 3. scikit library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a096e86-5c2c-4f49-9339-d2a7423fcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.med'], shuffle=True, random_state=42)\n",
    "texts, labels = data.data, data.target\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe3707-de21-45b1-ba3b-9326b193f657",
   "metadata": {},
   "source": [
    "# 4.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a689c7f9-51bc-463e-8025-b69db78cc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Gardening is my favourite hobby. I own a small plot of land next to our house. I cultivate gardening there. Every day, I spend half an hour gardening. After returning from my morning walk, I go to my garden with a spade and a bucket of water. I prepare the soil, prune the plants, and water them. I also use insecticides and fertilisers. My heart leaps with joy when I see the plants swaying in the wind. I feel heavenly pleasure as I watch them grow day by day.\") \n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfad56-5733-42cf-8719-69ef584eadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in doc.sents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a695b-b486-4c04-932f-cbeebd9d4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=((i.text  for i in doc if not i.is_stop))\n",
    "sent = (\" \".join(list1))\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fa5f8-1e12-4dcf-a83b-bd4b2188261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_doc=list(doc.sents)\n",
    "list_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f26d0a-a367-4f6b-acce-d368773c953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_doc:\n",
    "    for j in i:\n",
    "        if not j.is_stop:\n",
    "            print(j, type(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eeeae8-2480-421e-8f38-b3b93e43ce7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a150ea7-e085-47cc-b56c-2d0509875be0",
   "metadata": {},
   "source": [
    "# 5.tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48235a2e-ba70-4d02-b8aa-676b429c6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "d1 = ['Kabi is a bad boy','The bad thing is character','him character is very bad']\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "t=tfidf.fit_transform(d1)\n",
    "\n",
    "print(t.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee13b5-bff7-4914-aa7c-0d24f4c4a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.med'], shuffle=True, random_state=42)\n",
    "texts, labels = data.data, data.target\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c009b0-d941-43c9-b831-78b2b0798b93",
   "metadata": {},
   "source": [
    "# 6. naive bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1f909-d76d-4724-bd94-4eea190af630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.med'], shuffle=True, random_state=42)\n",
    "texts, labels = data.data, data.target\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d06fc9-4fd3-4364-82f8-8847bb3185f5",
   "metadata": {},
   "source": [
    "# 7. word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e1fcf-259b-4694-9d73-61037fc45476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0dc52-c441-462f-8a02-e7a29add8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample text\n",
    "text = \" Moulish data science\"\n",
    "\n",
    "# Create the word cloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65638555-c4ed-4d07-92a9-766cfb786569",
   "metadata": {},
   "source": [
    "# 8. python keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02204ca8-59a7-4ead-acfb-d35fb5c9c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE\n",
    "\n",
    "# Initialize RAKE by providing a stopwords list\n",
    "stopwords_path = 'SmartStoplist.txt'  # Path to the stopwords list file\n",
    "rake = RAKE.Rake(stopwords_path)\n",
    "\n",
    "# Sample text\n",
    "text = \"Automatic keyword extraction is useful for summarizing documents and identifying key terms.\"\n",
    "\n",
    "# Extract keywords\n",
    "keywords = rake.run(text)\n",
    "\n",
    "# Display the extracted keywords\n",
    "print(\"Keywords extracted:\", [keyword[0] for keyword in keywords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25199737-33fb-45d4-ba44-f712abe36896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Automatic keyword extraction is useful for summarizing documents and identifying key terms.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract noun chunks as keywords\n",
    "keywords = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "# Display the keywords\n",
    "print(\"Keywords extracted:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d48b14-c4d6-48e3-bc82-56964b4ab142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# Initialize KeyBERT model\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Sample text\n",
    "text = \"Automatic keyword extraction is useful for summarizing documents and identifying key terms.\"\n",
    "\n",
    "# Extract keywords\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)\n",
    "\n",
    "# Display the keywords\n",
    "print(\"Keywords extracted:\", [keyword[0] for keyword in keywords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e99a3b-5336-4a8e-a109-b83ea6b945c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58d93ffc-10e5-404c-b7ae-b421207de8dc",
   "metadata": {},
   "source": [
    "# 9. named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2700c97-dc0d-4054-8c17-0f9e7a66de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709ef6c-7f94-4e4c-946b-bcf90ee94a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "content = \"i am living at Salem my name is Moulish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b62d8c-85ef-4f35-8b62-7532dc2cbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345d7b7-3afe-43b5-86c2-d7de17444f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print (token.text,\"-->\",token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca273b-4b54-430c-8d9a-6837fc01d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3ce7e-28bd-4e0c-a561-a263bc1d6a2d",
   "metadata": {},
   "source": [
    "# 10. latent semantic analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1a137-d3a0-4862-9e6b-64fc5012c9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2ce73-16c0-4652-be40-3f260d2fd2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3dad9b-82c8-424f-9366-935a128ae975",
   "metadata": {},
   "source": [
    "# 11. optimum number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43bfea4-8aa8-44b2-b4ce-2d8b41f62f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"scipy<1.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc260e0c-52e4-4078-a854-7783e6978969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "documents = [\n",
    "    \"Natural language processing enables machines to understand human language.\",\n",
    "    \"Topic modeling helps in discovering abstract topics in a collection of documents.\",\n",
    "    \"Latent Dirichlet Allocation is a popular algorithm for topic modeling.\",\n",
    "    \"Text preprocessing is essential for effective NLP.\",\n",
    "    \"Data analysis is crucial for extracting meaningful insights.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d3a67-5c2a-4b67-8a7a-caad941d0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "texts = [[word for word in doc.lower().split() if word not in stop_words] for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "def compute_coherence_values(corpus, dictionary, texts, start, limit, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "start = 2\n",
    "limit = 10\n",
    "step = 1\n",
    "model_list, coherence_values = compute_coherence_values(corpus, dictionary, texts, start, limit, step)\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Scores for Different Number of Topics\")\n",
    "plt.show()\n",
    "\n",
    "optimal_idx = coherence_values.index(max(coherence_values)) + start\n",
    "print(f\"The optimal number of topics is: {optimal_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95f1a6-d7e1-4bfa-bd59-ea71546834f0",
   "metadata": {},
   "source": [
    "# 12. Fundamental of topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6fdfba-c06f-4641-a525-45af165cb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Dogs are great pets.\",\n",
    "    \"Cats and dogs are both popular pets.\",\n",
    "    \"Dogs are loyal companions.\",\n",
    "    \"Cats are independent animals.\"\n",
    "]\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "cleaned_docs = [[word for word in doc.lower().split() if word not in stop_words] for doc in documents]\n",
    "\n",
    "# Step 2: Vectorization\n",
    "dictionary = corpora.Dictionary(cleaned_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in cleaned_docs]\n",
    "\n",
    "# Step 3: Model Selection and Step 4: Model Training\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
    "\n",
    "# Step 5: Result Interpretation\n",
    "topics = lda_model.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a790d4-59fc-4eb3-bdbd-535cb68f0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "# Prepare the visualization\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display in Jupyter Notebook\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d49d0f-e350-4691-abfe-3f04e1c1a36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204b211-8ab3-4712-980c-95a5fc9f44dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba9972-848b-416f-89b7-93de5e346ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc80f4-47c7-43b4-b9ec-b90db311e050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf98fc-1b56-431a-a7ca-ffb7e070eca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd8276-0672-47fd-b0e5-ca84bef8c90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424f147-ec77-4a3e-84dd-220a69dd048a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6ffdc-2f95-4d16-a3af-af2634206c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc5eaa-67f5-4b19-b90d-74c02738a803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe65ba4-ea56-49ec-b785-fee8eb174dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8708cc-6fd2-4626-9279-66b6eb65a08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb9285-8619-41a3-b3fc-e4d3cce49e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a1cf9-63b0-4513-a838-d772ee92bdb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5248fbc-559a-4e35-a52f-e42f5d7273a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9334608-3ab0-4b55-948d-aab23ee43e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77adc31e-0046-4af4-9357-ec9016c26c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194915ee-ffd9-4800-adaa-78e503c9e80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dd774-fad8-4d11-ba91-e193b7fde1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
